{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNx3LxWSN46Acj3esjKaLBs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cerabbite/Easy-conalng/blob/main/Word.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each word in the top 10.000 most used English, Spanish, Japanese and Chinese words. There will be 100-1.000 sample sentences with those words (written in using the english alphabet). Using these sentences this script is going to work out how related each word is to another word."
      ],
      "metadata": {
        "id": "32YGG-XhZ2nR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cXnCOjDzXq84"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "#!pip install keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import json\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset to python\n",
        "with open('Sarcasm_Headlines_Dataset_v2.json', 'r') as f:\n",
        "  info = f.readlines()\n",
        "\n",
        "datastore = []\n",
        "for i in info:\n",
        "  datastore.append(json.loads(i))\n",
        "\n",
        "\n",
        "sentences = []\n",
        "labels = []\n",
        "urls = []\n",
        "for item in datastore:\n",
        "  sentences.append(item['headline'])\n",
        "  labels.append(item['is_sarcastic'])\n",
        "  urls.append(item['article_link'])"
      ],
      "metadata": {
        "id": "4mCSxt2EG-Nj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Variables\n",
        "vocab_size = 10000\n",
        "embedding_dim = 16\n",
        "max_length = 100\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<OOV>\"\n",
        "training_size = 20000"
      ],
      "metadata": {
        "id": "e38RPrkaAeiC"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seperate training from testing\n",
        "training_sentences = sentences[0:training_size]\n",
        "testing_sentences = sentences[training_size:]\n",
        "training_labels = labels[0:training_size]\n",
        "testing_labels = labels[training_size:]"
      ],
      "metadata": {
        "id": "lNTCN0yi7xGe"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "\"\"\"sentences = [\n",
        "             'I love my dog',\n",
        "             'I love my cat',\n",
        "             'You love my dog!',\n",
        "             'Do you think my dog is amazing?'\n",
        "]\"\"\"\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "#print(word_index)\n",
        "print(training_padded[0])\n",
        "print(training_padded.shape)"
      ],
      "metadata": {
        "id": "Q8theL_qZn-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74c83123-6da4-450e-a3a0-7131f614f561"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   1  325 3169 5817 2489    3  655  993    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "(20000, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
        "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "print(testing_padded[0])\n",
        "print(testing_padded.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2H2vOL1gfKSc",
        "outputId": "3afd985d-945b-40cf-855d-e4a4931261dc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  83 4338    1    6 2186  137  625  181   31   28 3008   56    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "(8619, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural network\n",
        "training_padded = np.array(training_padded)\n",
        "training_labels = np.array(training_labels)\n",
        "testing_padded = np.array(testing_padded)\n",
        "testing_labels = np.array(testing_labels)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "                             tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "                             tf.keras.layers.GlobalAveragePooling1D(),\n",
        "                             tf.keras.layers.Dense(24, activation='relu'),\n",
        "                             tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "num_epochs = 50\n",
        "\n",
        "history = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels), verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7XejK7y_aX5",
        "outputId": "800c8fa2-1aa2-4355-f07c-10249d4f9d0b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "625/625 - 4s - loss: 0.6525 - accuracy: 0.6210 - val_loss: 0.5117 - val_accuracy: 0.8015 - 4s/epoch - 6ms/step\n",
            "Epoch 2/50\n",
            "625/625 - 2s - loss: 0.3981 - accuracy: 0.8389 - val_loss: 0.3650 - val_accuracy: 0.8426 - 2s/epoch - 4ms/step\n",
            "Epoch 3/50\n",
            "625/625 - 2s - loss: 0.3022 - accuracy: 0.8778 - val_loss: 0.3478 - val_accuracy: 0.8459 - 2s/epoch - 4ms/step\n",
            "Epoch 4/50\n",
            "625/625 - 2s - loss: 0.2523 - accuracy: 0.9014 - val_loss: 0.3310 - val_accuracy: 0.8566 - 2s/epoch - 3ms/step\n",
            "Epoch 5/50\n",
            "625/625 - 2s - loss: 0.2181 - accuracy: 0.9179 - val_loss: 0.3337 - val_accuracy: 0.8550 - 2s/epoch - 3ms/step\n",
            "Epoch 6/50\n",
            "625/625 - 2s - loss: 0.1911 - accuracy: 0.9280 - val_loss: 0.3502 - val_accuracy: 0.8527 - 2s/epoch - 4ms/step\n",
            "Epoch 7/50\n",
            "625/625 - 2s - loss: 0.1698 - accuracy: 0.9376 - val_loss: 0.3644 - val_accuracy: 0.8494 - 2s/epoch - 4ms/step\n",
            "Epoch 8/50\n",
            "625/625 - 2s - loss: 0.1519 - accuracy: 0.9449 - val_loss: 0.3748 - val_accuracy: 0.8496 - 2s/epoch - 4ms/step\n",
            "Epoch 9/50\n",
            "625/625 - 3s - loss: 0.1372 - accuracy: 0.9522 - val_loss: 0.3965 - val_accuracy: 0.8465 - 3s/epoch - 4ms/step\n",
            "Epoch 10/50\n",
            "625/625 - 2s - loss: 0.1243 - accuracy: 0.9566 - val_loss: 0.4227 - val_accuracy: 0.8410 - 2s/epoch - 4ms/step\n",
            "Epoch 11/50\n",
            "625/625 - 2s - loss: 0.1125 - accuracy: 0.9617 - val_loss: 0.4469 - val_accuracy: 0.8383 - 2s/epoch - 3ms/step\n",
            "Epoch 12/50\n",
            "625/625 - 2s - loss: 0.1019 - accuracy: 0.9661 - val_loss: 0.4781 - val_accuracy: 0.8332 - 2s/epoch - 4ms/step\n",
            "Epoch 13/50\n",
            "625/625 - 2s - loss: 0.0942 - accuracy: 0.9685 - val_loss: 0.5020 - val_accuracy: 0.8321 - 2s/epoch - 4ms/step\n",
            "Epoch 14/50\n",
            "625/625 - 2s - loss: 0.0851 - accuracy: 0.9730 - val_loss: 0.5811 - val_accuracy: 0.8214 - 2s/epoch - 4ms/step\n",
            "Epoch 15/50\n",
            "625/625 - 2s - loss: 0.0794 - accuracy: 0.9739 - val_loss: 0.5730 - val_accuracy: 0.8263 - 2s/epoch - 3ms/step\n",
            "Epoch 16/50\n",
            "625/625 - 2s - loss: 0.0708 - accuracy: 0.9776 - val_loss: 0.6020 - val_accuracy: 0.8248 - 2s/epoch - 3ms/step\n",
            "Epoch 17/50\n",
            "625/625 - 2s - loss: 0.0656 - accuracy: 0.9786 - val_loss: 0.6297 - val_accuracy: 0.8252 - 2s/epoch - 3ms/step\n",
            "Epoch 18/50\n",
            "625/625 - 2s - loss: 0.0597 - accuracy: 0.9823 - val_loss: 0.6670 - val_accuracy: 0.8217 - 2s/epoch - 3ms/step\n",
            "Epoch 19/50\n",
            "625/625 - 2s - loss: 0.0559 - accuracy: 0.9825 - val_loss: 0.7086 - val_accuracy: 0.8195 - 2s/epoch - 3ms/step\n",
            "Epoch 20/50\n",
            "625/625 - 2s - loss: 0.0513 - accuracy: 0.9840 - val_loss: 0.7426 - val_accuracy: 0.8187 - 2s/epoch - 4ms/step\n",
            "Epoch 21/50\n",
            "625/625 - 3s - loss: 0.0456 - accuracy: 0.9871 - val_loss: 0.8037 - val_accuracy: 0.8139 - 3s/epoch - 5ms/step\n",
            "Epoch 22/50\n",
            "625/625 - 3s - loss: 0.0413 - accuracy: 0.9880 - val_loss: 0.8133 - val_accuracy: 0.8159 - 3s/epoch - 4ms/step\n",
            "Epoch 23/50\n",
            "625/625 - 2s - loss: 0.0405 - accuracy: 0.9877 - val_loss: 0.8578 - val_accuracy: 0.8140 - 2s/epoch - 3ms/step\n",
            "Epoch 24/50\n",
            "625/625 - 3s - loss: 0.0376 - accuracy: 0.9890 - val_loss: 0.8829 - val_accuracy: 0.8101 - 3s/epoch - 4ms/step\n",
            "Epoch 25/50\n",
            "625/625 - 3s - loss: 0.0348 - accuracy: 0.9897 - val_loss: 0.9160 - val_accuracy: 0.8133 - 3s/epoch - 4ms/step\n",
            "Epoch 26/50\n",
            "625/625 - 3s - loss: 0.0310 - accuracy: 0.9916 - val_loss: 0.9576 - val_accuracy: 0.8098 - 3s/epoch - 4ms/step\n",
            "Epoch 27/50\n",
            "625/625 - 3s - loss: 0.0270 - accuracy: 0.9930 - val_loss: 1.0427 - val_accuracy: 0.8065 - 3s/epoch - 5ms/step\n",
            "Epoch 28/50\n",
            "625/625 - 2s - loss: 0.0266 - accuracy: 0.9922 - val_loss: 1.0433 - val_accuracy: 0.8052 - 2s/epoch - 3ms/step\n",
            "Epoch 29/50\n",
            "625/625 - 2s - loss: 0.0268 - accuracy: 0.9918 - val_loss: 1.0983 - val_accuracy: 0.8024 - 2s/epoch - 4ms/step\n",
            "Epoch 30/50\n",
            "625/625 - 3s - loss: 0.0240 - accuracy: 0.9934 - val_loss: 1.2018 - val_accuracy: 0.7995 - 3s/epoch - 5ms/step\n",
            "Epoch 31/50\n",
            "625/625 - 3s - loss: 0.0210 - accuracy: 0.9948 - val_loss: 1.1774 - val_accuracy: 0.8031 - 3s/epoch - 4ms/step\n",
            "Epoch 32/50\n",
            "625/625 - 3s - loss: 0.0198 - accuracy: 0.9944 - val_loss: 1.2167 - val_accuracy: 0.8015 - 3s/epoch - 5ms/step\n",
            "Epoch 33/50\n",
            "625/625 - 3s - loss: 0.0170 - accuracy: 0.9956 - val_loss: 1.2691 - val_accuracy: 0.7996 - 3s/epoch - 4ms/step\n",
            "Epoch 34/50\n",
            "625/625 - 2s - loss: 0.0166 - accuracy: 0.9951 - val_loss: 1.3224 - val_accuracy: 0.7982 - 2s/epoch - 4ms/step\n",
            "Epoch 35/50\n",
            "625/625 - 2s - loss: 0.0156 - accuracy: 0.9958 - val_loss: 1.3620 - val_accuracy: 0.7995 - 2s/epoch - 4ms/step\n",
            "Epoch 36/50\n",
            "625/625 - 2s - loss: 0.0139 - accuracy: 0.9965 - val_loss: 1.4362 - val_accuracy: 0.7966 - 2s/epoch - 4ms/step\n",
            "Epoch 37/50\n",
            "625/625 - 3s - loss: 0.0125 - accuracy: 0.9970 - val_loss: 1.4573 - val_accuracy: 0.7951 - 3s/epoch - 4ms/step\n",
            "Epoch 38/50\n",
            "625/625 - 2s - loss: 0.0131 - accuracy: 0.9962 - val_loss: 1.5194 - val_accuracy: 0.7942 - 2s/epoch - 4ms/step\n",
            "Epoch 39/50\n",
            "625/625 - 3s - loss: 0.0117 - accuracy: 0.9971 - val_loss: 1.5472 - val_accuracy: 0.7963 - 3s/epoch - 4ms/step\n",
            "Epoch 40/50\n",
            "625/625 - 2s - loss: 0.0097 - accuracy: 0.9977 - val_loss: 1.5961 - val_accuracy: 0.7957 - 2s/epoch - 4ms/step\n",
            "Epoch 41/50\n",
            "625/625 - 2s - loss: 0.0131 - accuracy: 0.9957 - val_loss: 1.6783 - val_accuracy: 0.7949 - 2s/epoch - 4ms/step\n",
            "Epoch 42/50\n",
            "625/625 - 2s - loss: 0.0096 - accuracy: 0.9973 - val_loss: 1.6658 - val_accuracy: 0.7961 - 2s/epoch - 3ms/step\n",
            "Epoch 43/50\n",
            "625/625 - 2s - loss: 0.0082 - accuracy: 0.9980 - val_loss: 1.7131 - val_accuracy: 0.7908 - 2s/epoch - 4ms/step\n",
            "Epoch 44/50\n",
            "625/625 - 2s - loss: 0.0085 - accuracy: 0.9976 - val_loss: 1.7470 - val_accuracy: 0.7935 - 2s/epoch - 4ms/step\n",
            "Epoch 45/50\n",
            "625/625 - 2s - loss: 0.0080 - accuracy: 0.9980 - val_loss: 1.7828 - val_accuracy: 0.7895 - 2s/epoch - 4ms/step\n",
            "Epoch 46/50\n",
            "625/625 - 2s - loss: 0.0076 - accuracy: 0.9983 - val_loss: 1.8362 - val_accuracy: 0.7880 - 2s/epoch - 4ms/step\n",
            "Epoch 47/50\n",
            "625/625 - 2s - loss: 0.0075 - accuracy: 0.9979 - val_loss: 1.8760 - val_accuracy: 0.7914 - 2s/epoch - 4ms/step\n",
            "Epoch 48/50\n",
            "625/625 - 2s - loss: 0.0073 - accuracy: 0.9981 - val_loss: 1.8914 - val_accuracy: 0.7887 - 2s/epoch - 4ms/step\n",
            "Epoch 49/50\n",
            "625/625 - 2s - loss: 0.0064 - accuracy: 0.9985 - val_loss: 1.9216 - val_accuracy: 0.7903 - 2s/epoch - 4ms/step\n",
            "Epoch 50/50\n",
            "625/625 - 2s - loss: 0.0075 - accuracy: 0.9980 - val_loss: 2.0169 - val_accuracy: 0.7843 - 2s/epoch - 4ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use trained model\n",
        "sentence = [\n",
        "            \"Granny starting to fear spiders in the garden might be real\",\n",
        "            \"the weather today is bright and sunny\",\n",
        "            \"That's just what I needed today!\",\n",
        "            \"Well, what a surprise.\",\n",
        "            \"Really, Sherlock? No! You are clever.\",\n",
        "            \"Today I had a job interview.\",\n",
        "            \"He went to grab some snacks.\"\n",
        "]\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(sentence)\n",
        "\n",
        "padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "print(model.predict(padded))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db9NEnoyFKw6",
        "outputId": "ac525cc5-26e4-4474-dfd7-8faeb0636762"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[9.99617934e-01]\n",
            " [5.64974880e-06]\n",
            " [9.99994516e-01]\n",
            " [1.08725915e-04]\n",
            " [4.13626432e-04]\n",
            " [6.87621196e-08]\n",
            " [6.43782854e-01]]\n"
          ]
        }
      ]
    }
  ]
}