{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNxWgQINp4hzTEfbfY3ujwp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cerabbite/CGEL/blob/main/Word.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each word in the top 10.000 most used English, Spanish, Japanese and Chinese words. There will be 100-1.000 sample sentences with those words (written in using the english alphabet). Using these sentences this script is going to work out how related words are."
      ],
      "metadata": {
        "id": "32YGG-XhZ2nR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "cXnCOjDzXq84"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "#!pip install keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "\n",
        "data = open(\"irish poetry.txt\").read()\n",
        "corpus = data.lower().split(\"\\n\")\n",
        "\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1"
      ],
      "metadata": {
        "id": "52EKzVyx94-x"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences = []\n",
        "for line in corpus:\n",
        "  token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "  for i in range(1, len(token_list)):\n",
        "    n_gram_sequence = token_list[:i+1]\n",
        "    input_sequences.append(n_gram_sequence)"
      ],
      "metadata": {
        "id": "LASs-ZPe-7PA"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))"
      ],
      "metadata": {
        "id": "lcRUEJqZ_33e"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xs = input_sequences[:,:-1]\n",
        "labels = input_sequences[:,-1]\n",
        "\n",
        "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
      ],
      "metadata": {
        "id": "Lh3i1tL_AeAt"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(150)))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "adam = Adam(lr=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "history = model.fit(xs, ys, epochs=100, verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vi4QA8LpAtVu",
        "outputId": "473126c5-2b08-4667-dd99-e0ff42398432"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "377/377 [==============================] - 28s 63ms/step - loss: 6.6226 - accuracy: 0.0713\n",
            "Epoch 2/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 5.7393 - accuracy: 0.1160\n",
            "Epoch 3/100\n",
            "377/377 [==============================] - 24s 62ms/step - loss: 4.8469 - accuracy: 0.1708\n",
            "Epoch 4/100\n",
            "377/377 [==============================] - 24s 62ms/step - loss: 3.9110 - accuracy: 0.2400\n",
            "Epoch 5/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 3.0620 - accuracy: 0.3480\n",
            "Epoch 6/100\n",
            "377/377 [==============================] - 23s 62ms/step - loss: 2.4259 - accuracy: 0.4484\n",
            "Epoch 7/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 1.9389 - accuracy: 0.5470\n",
            "Epoch 8/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 1.6077 - accuracy: 0.6156\n",
            "Epoch 9/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 1.4160 - accuracy: 0.6560\n",
            "Epoch 10/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 1.2829 - accuracy: 0.6847\n",
            "Epoch 11/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 1.1720 - accuracy: 0.7109\n",
            "Epoch 12/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 1.0736 - accuracy: 0.7325\n",
            "Epoch 13/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 1.0352 - accuracy: 0.7412\n",
            "Epoch 14/100\n",
            "377/377 [==============================] - 24s 64ms/step - loss: 0.9940 - accuracy: 0.7487\n",
            "Epoch 15/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 0.9960 - accuracy: 0.7487\n",
            "Epoch 16/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 1.0281 - accuracy: 0.7357\n",
            "Epoch 17/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 1.0967 - accuracy: 0.7157\n",
            "Epoch 18/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 1.1640 - accuracy: 0.7009\n",
            "Epoch 19/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 1.0975 - accuracy: 0.7139\n",
            "Epoch 20/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 1.0545 - accuracy: 0.7255\n",
            "Epoch 21/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 1.0272 - accuracy: 0.7342\n",
            "Epoch 22/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 0.9907 - accuracy: 0.7419\n",
            "Epoch 23/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 0.9052 - accuracy: 0.7616\n",
            "Epoch 24/100\n",
            "377/377 [==============================] - 23s 62ms/step - loss: 0.8729 - accuracy: 0.7748\n",
            "Epoch 25/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 0.8553 - accuracy: 0.7770\n",
            "Epoch 26/100\n",
            "377/377 [==============================] - 23s 62ms/step - loss: 0.8792 - accuracy: 0.7740\n",
            "Epoch 27/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 1.0307 - accuracy: 0.7344\n",
            "Epoch 28/100\n",
            "377/377 [==============================] - 24s 64ms/step - loss: 1.0495 - accuracy: 0.7251\n",
            "Epoch 29/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 1.0152 - accuracy: 0.7337\n",
            "Epoch 30/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 0.9791 - accuracy: 0.7419\n",
            "Epoch 31/100\n",
            "377/377 [==============================] - 24s 62ms/step - loss: 0.9496 - accuracy: 0.7515\n",
            "Epoch 32/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 0.8738 - accuracy: 0.7732\n",
            "Epoch 33/100\n",
            "377/377 [==============================] - 24s 64ms/step - loss: 0.8532 - accuracy: 0.7766\n",
            "Epoch 34/100\n",
            "377/377 [==============================] - 24s 64ms/step - loss: 0.9196 - accuracy: 0.7616\n",
            "Epoch 35/100\n",
            "377/377 [==============================] - 24s 64ms/step - loss: 0.9517 - accuracy: 0.7494\n",
            "Epoch 36/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 0.9157 - accuracy: 0.7591\n",
            "Epoch 37/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 0.9422 - accuracy: 0.7506\n",
            "Epoch 38/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 0.9409 - accuracy: 0.7503\n",
            "Epoch 39/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 0.9539 - accuracy: 0.7477\n",
            "Epoch 40/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 0.9437 - accuracy: 0.7533\n",
            "Epoch 41/100\n",
            "377/377 [==============================] - 24s 64ms/step - loss: 0.8840 - accuracy: 0.7628\n",
            "Epoch 42/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 0.8465 - accuracy: 0.7751\n",
            "Epoch 43/100\n",
            "377/377 [==============================] - 24s 64ms/step - loss: 0.8217 - accuracy: 0.7789\n",
            "Epoch 44/100\n",
            "377/377 [==============================] - 24s 64ms/step - loss: 0.8643 - accuracy: 0.7666\n",
            "Epoch 45/100\n",
            "377/377 [==============================] - 24s 64ms/step - loss: 0.8988 - accuracy: 0.7599\n",
            "Epoch 46/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 0.9502 - accuracy: 0.7485\n",
            "Epoch 47/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 1.1976 - accuracy: 0.7028\n",
            "Epoch 48/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 1.0979 - accuracy: 0.7176\n",
            "Epoch 49/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 1.0133 - accuracy: 0.7328\n",
            "Epoch 50/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 0.9078 - accuracy: 0.7595\n",
            "Epoch 51/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 0.8046 - accuracy: 0.7863\n",
            "Epoch 52/100\n",
            "377/377 [==============================] - 23s 62ms/step - loss: 0.7508 - accuracy: 0.8020\n",
            "Epoch 53/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 0.7584 - accuracy: 0.7990\n",
            "Epoch 54/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 0.8125 - accuracy: 0.7863\n",
            "Epoch 55/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 0.9176 - accuracy: 0.7618\n",
            "Epoch 56/100\n",
            "377/377 [==============================] - 23s 62ms/step - loss: 0.9858 - accuracy: 0.7402\n",
            "Epoch 57/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 1.0054 - accuracy: 0.7370\n",
            "Epoch 58/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 1.0284 - accuracy: 0.7299\n",
            "Epoch 59/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 0.9524 - accuracy: 0.7505\n",
            "Epoch 60/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 0.9298 - accuracy: 0.7555\n",
            "Epoch 61/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 0.8901 - accuracy: 0.7686\n",
            "Epoch 62/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 0.8818 - accuracy: 0.7676\n",
            "Epoch 63/100\n",
            "377/377 [==============================] - 24s 64ms/step - loss: 0.8063 - accuracy: 0.7878\n",
            "Epoch 64/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 0.8254 - accuracy: 0.7873\n",
            "Epoch 65/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 0.8350 - accuracy: 0.7802\n",
            "Epoch 66/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 0.8510 - accuracy: 0.7771\n",
            "Epoch 67/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 0.8930 - accuracy: 0.7664\n",
            "Epoch 68/100\n",
            "377/377 [==============================] - 24s 64ms/step - loss: 0.9348 - accuracy: 0.7578\n",
            "Epoch 69/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 0.9677 - accuracy: 0.7524\n",
            "Epoch 70/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 0.9362 - accuracy: 0.7564\n",
            "Epoch 71/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 0.9216 - accuracy: 0.7567\n",
            "Epoch 72/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 0.8872 - accuracy: 0.7660\n",
            "Epoch 73/100\n",
            "377/377 [==============================] - 24s 64ms/step - loss: 0.8367 - accuracy: 0.7790\n",
            "Epoch 74/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 0.8254 - accuracy: 0.7824\n",
            "Epoch 75/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 0.8111 - accuracy: 0.7836\n",
            "Epoch 76/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 0.8568 - accuracy: 0.7766\n",
            "Epoch 77/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 0.8646 - accuracy: 0.7758\n",
            "Epoch 78/100\n",
            "377/377 [==============================] - 24s 64ms/step - loss: 0.8833 - accuracy: 0.7676\n",
            "Epoch 79/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 0.8866 - accuracy: 0.7652\n",
            "Epoch 80/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 0.9304 - accuracy: 0.7562\n",
            "Epoch 81/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 0.9713 - accuracy: 0.7529\n",
            "Epoch 82/100\n",
            "377/377 [==============================] - 24s 64ms/step - loss: 0.8990 - accuracy: 0.7642\n",
            "Epoch 83/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 0.8270 - accuracy: 0.7804\n",
            "Epoch 84/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 0.8127 - accuracy: 0.7890\n",
            "Epoch 85/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 0.7895 - accuracy: 0.7927\n",
            "Epoch 86/100\n",
            "377/377 [==============================] - 24s 64ms/step - loss: 0.8073 - accuracy: 0.7873\n",
            "Epoch 87/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 0.8326 - accuracy: 0.7876\n",
            "Epoch 88/100\n",
            "377/377 [==============================] - 24s 64ms/step - loss: 0.8627 - accuracy: 0.7750\n",
            "Epoch 89/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 0.8862 - accuracy: 0.7692\n",
            "Epoch 90/100\n",
            "377/377 [==============================] - 24s 64ms/step - loss: 0.9348 - accuracy: 0.7569\n",
            "Epoch 91/100\n",
            "377/377 [==============================] - 24s 63ms/step - loss: 1.0111 - accuracy: 0.7463\n",
            "Epoch 92/100\n",
            "377/377 [==============================] - 24s 65ms/step - loss: 0.9990 - accuracy: 0.7514\n",
            "Epoch 93/100\n",
            "377/377 [==============================] - 24s 64ms/step - loss: 0.9281 - accuracy: 0.7637\n",
            "Epoch 94/100\n",
            "377/377 [==============================] - 24s 64ms/step - loss: 0.8768 - accuracy: 0.7735\n",
            "Epoch 95/100\n",
            "377/377 [==============================] - 24s 64ms/step - loss: 0.8116 - accuracy: 0.7864\n",
            "Epoch 96/100\n",
            "377/377 [==============================] - 24s 64ms/step - loss: 0.7896 - accuracy: 0.7923\n",
            "Epoch 97/100\n",
            "377/377 [==============================] - 24s 64ms/step - loss: 0.7905 - accuracy: 0.7937\n",
            "Epoch 98/100\n",
            "377/377 [==============================] - 24s 64ms/step - loss: 0.8733 - accuracy: 0.7809\n",
            "Epoch 99/100\n",
            "377/377 [==============================] - 24s 64ms/step - loss: 0.9925 - accuracy: 0.7506\n",
            "Epoch 100/100\n",
            "377/377 [==============================] - 24s 64ms/step - loss: 1.0276 - accuracy: 0.7408\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "seed_text = \"I made a poetry machine\"\n",
        "next_words = 124\n",
        "\n",
        "for _ in range(next_words):\n",
        "  token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "  token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "  predicted = model.predict(token_list, verbose=0)\n",
        "  output_word = \"\"\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "    if index == predicted.all():\n",
        "      output_word = word\n",
        "      break\n",
        "  seed_text += f\" {output_word}\"\n",
        "\n",
        "print(seed_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLDrhih8DPIZ",
        "outputId": "45841701-78da-40b1-819a-0e8c43d8986c"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I made a poetry machine the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n"
          ]
        }
      ]
    }
  ]
}